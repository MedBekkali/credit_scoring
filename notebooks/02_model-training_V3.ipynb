{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-13T22:29:29.099975Z",
     "start_time": "2025-12-13T22:28:49.281810Z"
    }
   },
   "source": [
    "# # 02 - Entraînement des modèles de crédit scoring\n",
    "#\n",
    "# Objectifs :\n",
    "# - Entraîner plusieurs modèles (baseline + modèle avancé)\n",
    "# - Gérer le déséquilibre de classes\n",
    "# - Utiliser la validation croisée et l'AUC\n",
    "# - Définir une métrique métier (coût FN/FP)\n",
    "# - Optimiser le seuil de décision\n",
    "# - Tracker les expériences avec MLflow et exporter le modèle final\n",
    "\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# pour importer les modules du dossier src/\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.data_prep import build_datasets, build_preprocessor\n",
    "from src.metrics import compute_classic_metrics, business_cost, cost_curve\n",
    "\n",
    "# ## 1. Chargement des données et préparation de base\n",
    "\n",
    "\n",
    "train_df, test_df = build_datasets()\n",
    "\n",
    "X = train_df.drop(columns=[\"TARGET\"])\n",
    "y = train_df[\"TARGET\"]\n",
    "\n",
    "X.shape, y.shape\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((307511, 192), (307511,))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T22:29:29.188468Z",
     "start_time": "2025-12-13T22:29:29.151520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ## 2. Déséquilibre des classes\n",
    "\n",
    "n_pos = int((y == 1).sum())\n",
    "n_neg = int((y == 0).sum())\n",
    "imbalance_ratio = n_neg / n_pos\n",
    "\n",
    "print(\"Nombre de bons payeurs (0):\", n_neg)\n",
    "print(\"Nombre de mauvais payeurs (1):\", n_pos)\n",
    "print(\"Ratio négatifs/positifs (scale_pos_weight):\", imbalance_ratio)"
   ],
   "id": "25f185b50229aaa6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de bons payeurs (0): 282686\n",
      "Nombre de mauvais payeurs (1): 24825\n",
      "Ratio négatifs/positifs (scale_pos_weight): 11.387150050352467\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T22:29:29.376932Z",
     "start_time": "2025-12-13T22:29:29.201007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ## 3. Échantillon pour l'expérimentation\n",
    "#\n",
    "# Pour limiter le temps de calcul, la validation croisée est faite\n",
    "# sur un échantillon stratifié de 50 000 clients.\n",
    "# Le modèle final sera ensuite ré-entraîné sur toutes les données.\n",
    "\n",
    "\n",
    "sample_size = 50000\n",
    "X_small = X.sample(n=min(sample_size, len(X)), random_state=42)\n",
    "y_small = y.loc[X_small.index]\n",
    "\n",
    "X_small.shape, y_small.shape"
   ],
   "id": "23c2fd53d87c44a0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 192), (50000,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T22:29:29.474664Z",
     "start_time": "2025-12-13T22:29:29.402318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ## 4. Définition des modèles\n",
    "#\n",
    "# - Régression logistique (baseline)\n",
    "# - LightGBM (modèle avancé)\n",
    "\n",
    "\n",
    "logreg_model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "lgbm_model = LGBMClassifier(\n",
    "    n_estimators=200,  # valeur modérée pour limiter le temps de calcul\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"binary\",\n",
    "    scale_pos_weight=imbalance_ratio,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "models = {\n",
    "    \"logreg_baseline\": logreg_model,\n",
    "    \"lgbm_baseline\": lgbm_model,\n",
    "}\n",
    "\n",
    "models"
   ],
   "id": "f4c3ff0578119312",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logreg_baseline': LogisticRegression(class_weight='balanced', max_iter=1000, n_jobs=-1),\n",
       " 'lgbm_baseline': LGBMClassifier(colsample_bytree=0.8, learning_rate=0.05, n_estimators=200,\n",
       "                n_jobs=-1, objective='binary', random_state=42,\n",
       "                scale_pos_weight=11.387150050352467, subsample=0.8)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T22:34:49.397967Z",
     "start_time": "2025-12-13T22:32:55.330154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # 02 - Entraînement des modèles de crédit scoring\n",
    "#\n",
    "# Objectifs :\n",
    "# - Entraîner plusieurs modèles (baseline + modèle avancé)\n",
    "# - Gérer le déséquilibre de classes\n",
    "# - Utiliser la validation croisée et l'AUC\n",
    "# - Définir une métrique métier (coût FN/FP)\n",
    "# - Optimiser le seuil de décision\n",
    "# - Tracker les expériences avec MLflow et exporter le modèle final\n",
    "\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# pour importer les modules du dossier src/\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.data_prep import build_datasets, build_preprocessor\n",
    "from src.metrics import compute_classic_metrics, business_cost, cost_curve\n",
    "\n",
    "# ## 1. Chargement des données et préparation de base\n",
    "\n",
    "\n",
    "train_df, test_df = build_datasets()\n",
    "\n",
    "X = train_df.drop(columns=[\"TARGET\"])\n",
    "y = train_df[\"TARGET\"]\n",
    "\n",
    "X.shape, y.shape\n",
    "\n",
    "preprocessor = build_preprocessor(train_df)\n",
    "preprocessor\n",
    "\n",
    "# ## 2. Déséquilibre des classes\n",
    "\n",
    "\n",
    "n_pos = int((y == 1).sum())\n",
    "n_neg = int((y == 0).sum())\n",
    "imbalance_ratio = n_neg / n_pos\n",
    "\n",
    "print(\"Nombre de bons payeurs (0):\", n_neg)\n",
    "print(\"Nombre de mauvais payeurs (1):\", n_pos)\n",
    "print(\"Ratio négatifs/positifs (scale_pos_weight):\", imbalance_ratio)\n",
    "\n",
    "# ## 3. Échantillon pour l'expérimentation\n",
    "#\n",
    "# Pour limiter le temps de calcul, la validation croisée est faite\n",
    "# sur un échantillon stratifié de 50 000 clients.\n",
    "# Le modèle final sera ensuite ré-entraîné sur toutes les données.\n",
    "\n",
    "\n",
    "sample_size = 50000\n",
    "X_small = X.sample(n=min(sample_size, len(X)), random_state=42)\n",
    "y_small = y.loc[X_small.index]\n",
    "\n",
    "X_small.shape, y_small.shape\n",
    "\n",
    "# ## 4. Définition des modèles\n",
    "#\n",
    "# - Régression logistique (baseline)\n",
    "# - LightGBM (modèle avancé)\n",
    "\n",
    "\n",
    "logreg_model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "lgbm_model = LGBMClassifier(\n",
    "    n_estimators=200,  # valeur modérée pour limiter le temps de calcul\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"binary\",\n",
    "    scale_pos_weight=imbalance_ratio,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "models = {\n",
    "    \"logreg_baseline\": logreg_model,\n",
    "    \"lgbm_baseline\": lgbm_model,\n",
    "}\n",
    "\n",
    "models\n",
    "\n",
    "# ## 5. Validation croisée (AUC) + tracking MLflow\n",
    "#\n",
    "# - StratifiedKFold pour respecter le déséquilibre\n",
    "# - AUC comme métrique principale\n",
    "# - Tracking MLflow : paramètres, métriques, modèle\n",
    "\n",
    "\n",
    "# IMPORTANT : on pointe sur le mlruns à la racine du projet\n",
    "mlflow.set_tracking_uri(\"file:../mlruns\")\n",
    "mlflow.set_experiment(\"credit_scoring\")\n",
    "\n",
    "mlflow.get_tracking_uri()\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "results_auc = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n=== Modèle : {name} ===\")\n",
    "\n",
    "    pipeline = Pipeline(steps=[\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"model\", model),\n",
    "    ])\n",
    "\n",
    "    with mlflow.start_run(run_name=name):\n",
    "        # Validation croisée sur l'échantillon\n",
    "        scores = cross_val_score(\n",
    "            pipeline,\n",
    "            X_small,\n",
    "            y_small,\n",
    "            cv=cv,\n",
    "            scoring=\"roc_auc\",\n",
    "            n_jobs=1,  # évite les problèmes de parallélisation sur Windows\n",
    "        )\n",
    "        auc_mean = float(scores.mean())\n",
    "        auc_std = float(scores.std())\n",
    "\n",
    "        results_auc[name] = (auc_mean, auc_std)\n",
    "\n",
    "        # Log des principaux hyperparamètres\n",
    "        mlflow.log_param(\"model_type\", name)\n",
    "\n",
    "        if isinstance(model, LogisticRegression):\n",
    "            mlflow.log_param(\"max_iter\", model.max_iter)\n",
    "            mlflow.log_param(\"class_weight\", \"balanced\")\n",
    "\n",
    "        if isinstance(model, LGBMClassifier):\n",
    "            mlflow.log_param(\"n_estimators\", model.n_estimators)\n",
    "            mlflow.log_param(\"learning_rate\", model.learning_rate)\n",
    "            mlflow.log_param(\"num_leaves\", model.num_leaves)\n",
    "            mlflow.log_param(\"subsample\", model.subsample)\n",
    "            mlflow.log_param(\"colsample_bytree\", model.colsample_bytree)\n",
    "            mlflow.log_param(\"scale_pos_weight\", imbalance_ratio)\n",
    "\n",
    "        # Log des métriques de CV\n",
    "        mlflow.log_metric(\"cv_auc_mean\", auc_mean)\n",
    "        mlflow.log_metric(\"cv_auc_std\", auc_std)\n",
    "\n",
    "        # Entraîne le pipeline sur tout l'échantillon pour logguer un modèle\n",
    "        pipeline.fit(X_small, y_small)\n",
    "        mlflow.sklearn.log_model(pipeline, artifact_path=\"model\")\n",
    "\n",
    "        print(\"AUC moyenne (3-fold):\", auc_mean)\n",
    "        print(\"Écart-type AUC:\", auc_std)\n",
    "\n",
    "results_auc\n"
   ],
   "id": "1d9e17bdc29807c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de bons payeurs (0): 282686\n",
      "Nombre de mauvais payeurs (1): 24825\n",
      "Ratio négatifs/positifs (scale_pos_weight): 11.387150050352467\n",
      "\n",
      "=== Modèle : logreg_baseline ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/13 23:34:27 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC moyenne (3-fold): 0.6347927347356171\n",
      "Écart-type AUC: 0.005508871059131385\n",
      "\n",
      "=== Modèle : lgbm_baseline ===\n",
      "[LightGBM] [Info] Number of positive: 2682, number of negative: 30651\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038368 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21349\n",
      "[LightGBM] [Info] Number of data points in the train set: 33333, number of used features: 285\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080461 -> initscore=-2.436103\n",
      "[LightGBM] [Info] Start training from score -2.436103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Moham\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2682, number of negative: 30651\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027120 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21337\n",
      "[LightGBM] [Info] Number of data points in the train set: 33333, number of used features: 285\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080461 -> initscore=-2.436103\n",
      "[LightGBM] [Info] Start training from score -2.436103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Moham\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2682, number of negative: 30652\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035422 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21342\n",
      "[LightGBM] [Info] Number of data points in the train set: 33334, number of used features: 285\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080458 -> initscore=-2.436135\n",
      "[LightGBM] [Info] Start training from score -2.436135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Moham\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4023, number of negative: 45977\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042391 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 22067\n",
      "[LightGBM] [Info] Number of data points in the train set: 50000, number of used features: 287\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080460 -> initscore=-2.436113\n",
      "[LightGBM] [Info] Start training from score -2.436113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/13 23:34:45 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC moyenne (3-fold): 0.7448715111676378\n",
      "Écart-type AUC: 0.004166649015972984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'logreg_baseline': (0.6347927347356171, 0.005508871059131385),\n",
       " 'lgbm_baseline': (0.7448715111676378, 0.004166649015972984)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T22:35:02.653966Z",
     "start_time": "2025-12-13T22:35:01.842020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ## 6. Split train/validation pour le modèle final LightGBM\n",
    "\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "X_train.shape, X_valid.shape"
   ],
   "id": "c63aaba827122983",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((246008, 192), (61503, 192))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T22:35:51.047225Z",
     "start_time": "2025-12-13T22:35:33.047894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ## 7. Pipeline LightGBM final (sur le train) et métriques classiques\n",
    "lgbm_final = LGBMClassifier(\n",
    "    n_estimators=500,  # un peu plus élevé pour le modèle final\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=31,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"binary\",\n",
    "    scale_pos_weight=imbalance_ratio,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "final_pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", lgbm_final),\n",
    "])\n",
    "final_pipeline\n",
    "\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "# Probabilités de défaut sur le jeu de validation\n",
    "y_valid_proba = final_pipeline.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "# Métriques classiques pour le seuil 0.5\n",
    "metrics_05 = compute_classic_metrics(\n",
    "    y_true=y_valid,\n",
    "    y_proba=y_valid_proba,\n",
    "    threshold=0.5,\n",
    ")\n",
    "cost_05, conf_05 = business_cost(\n",
    "    y_true=y_valid,\n",
    "    y_proba=y_valid_proba,\n",
    "    threshold=0.5,\n",
    "    cost_fn=10.0,\n",
    "    cost_fp=1.0,\n",
    "    normalize=True,\n",
    ")\n",
    "metrics_05, cost_05, conf_05"
   ],
   "id": "78fca3c9223f37bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 19860, number of negative: 226148\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.192220 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 23444\n",
      "[LightGBM] [Info] Number of data points in the train set: 246008, number of used features: 298\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080729 -> initscore=-2.432482\n",
      "[LightGBM] [Info] Start training from score -2.432482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Moham\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'auc': np.float64(0.7679064000196357),\n",
       "  'precision': 0.17520187548840843,\n",
       "  'recall': 0.6773413897280967,\n",
       "  'f1': 0.27839403973509935},\n",
       " np.float64(0.5178934360925483),\n",
       " {'tn': np.int64(40706),\n",
       "  'fp': np.int64(15832),\n",
       "  'fn': np.int64(1602),\n",
       "  'tp': np.int64(3363)})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fineweb)",
   "language": "python",
   "name": "fineweb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
