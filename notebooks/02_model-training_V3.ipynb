{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# # 02 - Entraînement des modèles de crédit scoring\n",
    "#\n",
    "# Objectifs :\n",
    "# - Entraîner plusieurs modèles (baseline + modèle avancé)\n",
    "# - Gérer le déséquilibre de classes\n",
    "# - Utiliser la validation croisée et l'AUC\n",
    "# - Définir une métrique métier (coût FN/FP)\n",
    "# - Optimiser le seuil de décision\n",
    "# - Tracker les expériences avec MLflow et exporter le modèle final\n",
    "\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# pour importer les modules du dossier src/\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.data_prep import build_datasets, build_preprocessor\n",
    "from src.metrics import compute_classic_metrics, business_cost, cost_curve\n",
    "\n",
    "# ## 1. Chargement des données et préparation de base\n",
    "\n",
    "\n",
    "train_df, test_df = build_datasets()\n",
    "\n",
    "X = train_df.drop(columns=[\"TARGET\"])\n",
    "y = train_df[\"TARGET\"]\n",
    "\n",
    "X.shape, y.shape\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ## 2. Déséquilibre des classes\n",
    "\n",
    "n_pos = int((y == 1).sum())\n",
    "n_neg = int((y == 0).sum())\n",
    "imbalance_ratio = n_neg / n_pos\n",
    "\n",
    "print(\"Nombre de bons payeurs (0):\", n_neg)\n",
    "print(\"Nombre de mauvais payeurs (1):\", n_pos)\n",
    "print(\"Ratio négatifs/positifs (scale_pos_weight):\", imbalance_ratio)"
   ],
   "id": "25f185b50229aaa6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ## 3. Échantillon pour l'expérimentation\n",
    "#\n",
    "# Pour limiter le temps de calcul, la validation croisée est faite\n",
    "# sur un échantillon stratifié de 50 000 clients.\n",
    "# Le modèle final sera ensuite ré-entraîné sur toutes les données.\n",
    "\n",
    "\n",
    "sample_size = 50000\n",
    "X_small = X.sample(n=min(sample_size, len(X)), random_state=42)\n",
    "y_small = y.loc[X_small.index]\n",
    "\n",
    "X_small.shape, y_small.shape"
   ],
   "id": "23c2fd53d87c44a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ## 4. Définition des modèles\n",
    "#\n",
    "# - Régression logistique (baseline)\n",
    "# - LightGBM (modèle avancé)\n",
    "\n",
    "\n",
    "logreg_model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "lgbm_model = LGBMClassifier(\n",
    "    n_estimators=200,  # valeur modérée pour limiter le temps de calcul\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"binary\",\n",
    "    scale_pos_weight=imbalance_ratio,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "models = {\n",
    "    \"logreg_baseline\": logreg_model,\n",
    "    \"lgbm_baseline\": lgbm_model,\n",
    "}\n",
    "\n",
    "models"
   ],
   "id": "f4c3ff0578119312",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # 02 - Entraînement des modèles de crédit scoring\n",
    "#\n",
    "# Objectifs :\n",
    "# - Entraîner plusieurs modèles (baseline + modèle avancé)\n",
    "# - Gérer le déséquilibre de classes\n",
    "# - Utiliser la validation croisée et l'AUC\n",
    "# - Définir une métrique métier (coût FN/FP)\n",
    "# - Optimiser le seuil de décision\n",
    "# - Tracker les expériences avec MLflow et exporter le modèle final\n",
    "\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# pour importer les modules du dossier src/\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.data_prep import build_datasets, build_preprocessor\n",
    "from src.metrics import compute_classic_metrics, business_cost, cost_curve\n",
    "\n",
    "# ## 1. Chargement des données et préparation de base\n",
    "\n",
    "\n",
    "train_df, test_df = build_datasets()\n",
    "\n",
    "X = train_df.drop(columns=[\"TARGET\"])\n",
    "y = train_df[\"TARGET\"]\n",
    "\n",
    "X.shape, y.shape\n",
    "\n",
    "preprocessor = build_preprocessor(train_df)\n",
    "preprocessor\n",
    "\n",
    "# ## 2. Déséquilibre des classes\n",
    "\n",
    "\n",
    "n_pos = int((y == 1).sum())\n",
    "n_neg = int((y == 0).sum())\n",
    "imbalance_ratio = n_neg / n_pos\n",
    "\n",
    "print(\"Nombre de bons payeurs (0):\", n_neg)\n",
    "print(\"Nombre de mauvais payeurs (1):\", n_pos)\n",
    "print(\"Ratio négatifs/positifs (scale_pos_weight):\", imbalance_ratio)\n",
    "\n",
    "# ## 3. Échantillon pour l'expérimentation\n",
    "#\n",
    "# Pour limiter le temps de calcul, la validation croisée est faite\n",
    "# sur un échantillon stratifié de 50 000 clients.\n",
    "# Le modèle final sera ensuite ré-entraîné sur toutes les données.\n",
    "\n",
    "\n",
    "sample_size = 50000\n",
    "X_small = X.sample(n=min(sample_size, len(X)), random_state=42)\n",
    "y_small = y.loc[X_small.index]\n",
    "\n",
    "X_small.shape, y_small.shape\n",
    "\n",
    "# ## 4. Définition des modèles\n",
    "#\n",
    "# - Régression logistique (baseline)\n",
    "# - LightGBM (modèle avancé)\n",
    "\n",
    "\n",
    "logreg_model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "lgbm_model = LGBMClassifier(\n",
    "    n_estimators=200,  # valeur modérée pour limiter le temps de calcul\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"binary\",\n",
    "    scale_pos_weight=imbalance_ratio,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "models = {\n",
    "    \"logreg_baseline\": logreg_model,\n",
    "    \"lgbm_baseline\": lgbm_model,\n",
    "}\n",
    "\n",
    "models\n",
    "\n",
    "# ## 5. Validation croisée (AUC) + tracking MLflow\n",
    "#\n",
    "# - StratifiedKFold pour respecter le déséquilibre\n",
    "# - AUC comme métrique principale\n",
    "# - Tracking MLflow : paramètres, métriques, modèle\n",
    "\n",
    "\n",
    "# IMPORTANT : on pointe sur le mlruns à la racine du projet\n",
    "mlflow.set_tracking_uri(\"file:../mlruns\")\n",
    "mlflow.set_experiment(\"credit_scoring\")\n",
    "\n",
    "mlflow.get_tracking_uri()\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "results_auc = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n=== Modèle : {name} ===\")\n",
    "\n",
    "    pipeline = Pipeline(steps=[\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"model\", model),\n",
    "    ])\n",
    "\n",
    "    with mlflow.start_run(run_name=name):\n",
    "        # Validation croisée sur l'échantillon\n",
    "        scores = cross_val_score(\n",
    "            pipeline,\n",
    "            X_small,\n",
    "            y_small,\n",
    "            cv=cv,\n",
    "            scoring=\"roc_auc\",\n",
    "            n_jobs=1,  # évite les problèmes de parallélisation sur Windows\n",
    "        )\n",
    "        auc_mean = float(scores.mean())\n",
    "        auc_std = float(scores.std())\n",
    "\n",
    "        results_auc[name] = (auc_mean, auc_std)\n",
    "\n",
    "        # Log des principaux hyperparamètres\n",
    "        mlflow.log_param(\"model_type\", name)\n",
    "\n",
    "        if isinstance(model, LogisticRegression):\n",
    "            mlflow.log_param(\"max_iter\", model.max_iter)\n",
    "            mlflow.log_param(\"class_weight\", \"balanced\")\n",
    "\n",
    "        if isinstance(model, LGBMClassifier):\n",
    "            mlflow.log_param(\"n_estimators\", model.n_estimators)\n",
    "            mlflow.log_param(\"learning_rate\", model.learning_rate)\n",
    "            mlflow.log_param(\"num_leaves\", model.num_leaves)\n",
    "            mlflow.log_param(\"subsample\", model.subsample)\n",
    "            mlflow.log_param(\"colsample_bytree\", model.colsample_bytree)\n",
    "            mlflow.log_param(\"scale_pos_weight\", imbalance_ratio)\n",
    "\n",
    "        # Log des métriques de CV\n",
    "        mlflow.log_metric(\"cv_auc_mean\", auc_mean)\n",
    "        mlflow.log_metric(\"cv_auc_std\", auc_std)\n",
    "\n",
    "        # Entraîne le pipeline sur tout l'échantillon pour logguer un modèle\n",
    "        pipeline.fit(X_small, y_small)\n",
    "        mlflow.sklearn.log_model(pipeline, artifact_path=\"model\")\n",
    "\n",
    "        print(\"AUC moyenne (3-fold):\", auc_mean)\n",
    "        print(\"Écart-type AUC:\", auc_std)\n",
    "\n",
    "results_auc\n"
   ],
   "id": "1d9e17bdc29807c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ## 6. Split train/validation pour le modèle final LightGBM\n",
    "\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "X_train.shape, X_valid.shape"
   ],
   "id": "c63aaba827122983",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T23:10:37.071153Z",
     "start_time": "2025-12-13T23:10:14.054980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ## 7. Pipeline LightGBM final (sur le train) et métriques classiques\n",
    "lgbm_final = LGBMClassifier(\n",
    "    n_estimators=500,  # un peu plus élevé pour le modèle final\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=31,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective=\"binary\",\n",
    "    scale_pos_weight=imbalance_ratio,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "final_pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", lgbm_final),\n",
    "])\n",
    "final_pipeline\n",
    "\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "# Probabilités de défaut sur le jeu de validation\n",
    "y_valid_proba = final_pipeline.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "# Métriques classiques pour le seuil 0.5\n",
    "metrics_05 = compute_classic_metrics(\n",
    "    y_true=y_valid,\n",
    "    y_proba=y_valid_proba,\n",
    "    threshold=0.5,\n",
    ")\n",
    "cost_05, conf_05 = business_cost(\n",
    "    y_true=y_valid,\n",
    "    y_proba=y_valid_proba,\n",
    "    threshold=0.5,\n",
    "    cost_fn=10.0,\n",
    "    cost_fp=1.0,\n",
    "    normalize=True,\n",
    ")\n",
    "metrics_05, cost_05, conf_05"
   ],
   "id": "78fca3c9223f37bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 19860, number of negative: 226148\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.218058 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 23444\n",
      "[LightGBM] [Info] Number of data points in the train set: 246008, number of used features: 298\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080729 -> initscore=-2.432482\n",
      "[LightGBM] [Info] Start training from score -2.432482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Moham\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'auc': np.float64(0.7679064000196357),\n",
       "  'precision': 0.17520187548840843,\n",
       "  'recall': 0.6773413897280967,\n",
       "  'f1': 0.27839403973509935},\n",
       " np.float64(0.5178934360925483),\n",
       " {'tn': np.int64(40706),\n",
       "  'fp': np.int64(15832),\n",
       "  'fn': np.int64(1602),\n",
       "  'tp': np.int64(3363)})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fineweb)",
   "language": "python",
   "name": "fineweb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
